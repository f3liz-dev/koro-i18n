# koro-i18n Technical Flows

This document describes all technical flows in the koro-i18n system, covering frontend, backend, and GitHub Action interactions.

## Table of Contents

- [Upload Flow](#upload-flow)
- [Translation Flow](#translation-flow)
- [Display Flow](#display-flow)
- [Validation Flow](#validation-flow)
- [Cleanup Flow](#cleanup-flow)
- [Authentication Flows](#authentication-flows)

---

## Upload Flow

### Overview

The upload flow handles translations from GitHub repositories to the platform using OIDC authentication.

### Client Library Flow

**Location**: `client-library/src/index.ts`

1. **Configuration Loading**
   - Read `.koro-i18n.repo.config.toml`
   - Parse project name, platform URL, source/target languages
   - Support both direct file paths and glob patterns with `{lang}` marker

2. **File Discovery**
   - Scan repository for translation files
   - Support glob patterns: `locales/{lang}/**/*.json`
   - Auto-detect language codes from file paths
   - Filter files based on include/exclude patterns

3. **Git Blame Extraction**
   ```
   For each file:
     - Run `git blame --line-porcelain <file>`
     - Parse commit SHA, author, email, date per line
     - Map each translation key to its git blame info
   ```

4. **Metadata Generation**
   ```javascript
   metadata = {
     gitBlame: {
       "key": { commit, author, email, date }
     },
     charRanges: {
       "key": { start: [line, char], end: [line, char] }
     },
     sourceHashes: {
       "key": "hash123" // SHA256 hash (16 chars)
     }
   }
   ```

5. **Pre-Packing (CPU Optimization)**
   ```
   - Encode metadata with MessagePack
   - Convert to base64 string
   - Server just decodes base64 (zero encoding CPU)
   - Keeps worker under 10ms CPU limit
   ```

6. **Differential Upload**
   ```
   - Fetch existing files: GET /api/projects/:project/files/list-oidc
   - Compare sourceHash values
   - Skip unchanged files
   - Track all source files (for cleanup)
   ```

7. **Chunked Upload**
   ```
   If files > UPLOAD_CHUNK_SIZE (default: 10):
     - Split files into chunks
     - Each chunk: independent upload
     - Track: uploadId, chunkIndex, totalChunks, isLastChunk
     - Progress reporting per chunk
   ```

8. **Upload Execution**
   ```
   For each chunk:
     POST /api/projects/:project/upload
     Headers:
       Authorization: Bearer <OIDC_TOKEN>
       Content-Type: application/json
     Body: {
       branch: "main",
       commitSha: "abc123",
       sourceLanguage: "en",
       files: [
         {
           lang: "en",
           filename: "common.json",
           contents: { "key": "value" }, // Flattened
           metadata: "<base64-msgpack>",  // Pre-packed
           sourceHash: "file-hash"
         }
       ],
       chunked?: {
         uploadId, chunkIndex, totalChunks, isLastChunk
       },
       allSourceFiles: ["en/common.json", ...]  // For cleanup
     }
   ```

9. **Cleanup Call (Last Chunk or Non-Chunked)**
   ```
   POST /api/projects/:project/cleanup
   Body: {
     branch: "main",
     allSourceFiles: ["en/common.json", ...]
   }
   ```

### GitHub Action Flow

**Location**: `.github/actions/upload-translations/action.yml`

1. **Setup**
   ```yaml
   - Checkout repository with full history (fetch-depth: 0)
   - Setup Node.js 20
   - Build client library from source (no npm install needed)
   ```

2. **OIDC Token Acquisition**
   ```
   - GitHub Actions automatically provides OIDC token
   - Token includes repository info (org, repo, ref)
   - No secrets needed!
   ```

3. **Upload Execution**
   ```bash
   node client-library/dist/cli.js
   Environment:
     - PROJECT_NAME: from action input
     - PLATFORM_URL: from action input or default
     - GITHUB_TOKEN: automatic OIDC token
   ```

4. **Output**
   ```
   ðŸ“¦ Processing files for <project>...
   ðŸ” Checking for existing files...
   âœ¨ Skipping X unchanged files (differential upload)
   ðŸ“¦ Pre-packing Y files for optimized upload...
   ðŸ“¤ Uploading Y files (chunk size: 10)...
   âœ… Upload successful
   ðŸ§¹ Cleaned up Z orphaned files
   ```

### Backend Upload Flow

**Location**: `src/routes/project-files.ts`

1. **Request Validation**
   ```typescript
   - Verify OIDC token
   - Extract project info from token
   - Validate project access
   - Parse upload payload
   ```

2. **R2 Storage** (`src/lib/r2-storage.ts`)
   ```
   For each file:
     - Decode base64 pre-packed data
     - Generate R2 key: `[project]-[lang]-[filename]`
     - Store to R2 bucket (overwrite if exists)
     - R2 write: ~0.2ms CPU per file
   ```

3. **D1 Index Update (Batched)**
   ```sql
   -- Single batch operation for all files in chunk
   INSERT INTO R2File (projectId, language, filename, r2Key, sourceHash, ...)
   VALUES (...), (...), (...)
   ON CONFLICT(projectId, language, filename, branch) 
   DO UPDATE SET sourceHash = excluded.sourceHash, ...
   
   -- Batched operation: ~2ms CPU for 10 files
   -- vs individual upserts: ~10ms CPU for 10 files
   ```

4. **Translation Invalidation (Last Chunk Only)**
   ```
   If isLastChunk or non-chunked:
     For each file with changed sourceHash:
       UPDATE WebTranslation 
       SET isValid = false
       WHERE projectId = ? AND language = ? AND filename = ?
         AND sourceHash != (source hash from R2)
   ```

5. **Response**
   ```json
   {
     "success": true,
     "filesUploaded": 10,
     "chunked": {
       "chunkIndex": 1,
       "totalChunks": 24,
       "isLastChunk": false
     }
   }
   ```

### Cleanup Flow

**Location**: `src/lib/r2-storage.ts` - `cleanupOrphanedFiles()`

1. **Query Existing Files**
   ```sql
   SELECT language, filename FROM R2File
   WHERE projectId = ? AND branch = ?
   ```

2. **Compare with Source Files**
   ```typescript
   const orphaned = existingFiles.filter(
     file => !allSourceFiles.includes(`${file.language}/${file.filename}`)
   );
   ```

3. **Delete Orphaned Files**
   ```
   For each orphaned file:
     - Delete from R2 bucket
     - Delete from D1 index
     - Return list of deleted files
   ```

4. **Response**
   ```json
   {
     "deleted": 3,
     "files": ["en/old-file.json", "ja/removed.json"]
   }
   ```

---

## Translation Flow

### Frontend: Create Translation

**Location**: `src/app/pages/TranslationEditorPage.tsx`

1. **User Input**
   ```
   - User selects translation key
   - Enters translation value
   - Clicks save
   ```

2. **API Call** (`src/app/utils/translationApi.ts`)
   ```typescript
   await submitTranslation(projectId, language, filename, key, value)
   
   POST /api/translations
   Body: {
     projectId, language, filename, key, value
   }
   ```

### Backend: Create Translation

**Location**: `src/routes/translations.ts`

1. **Fetch Source Hash**
   ```typescript
   // Get R2 file to extract sourceHash for the key
   const r2File = await getR2File(projectId, sourceLanguage, filename);
   const sourceHash = r2File.metadata.sourceHashes[key];
   ```

2. **Create Web Translation**
   ```typescript
   await prisma.webTranslation.create({
     data: {
       projectId,
       language,
       filename,
       key,
       value,
       sourceHash,    // From R2
       isValid: true, // Initially valid
       status: 'pending',
       userId,
       username,
       avatarUrl
     }
   });
   ```

3. **Create History Entry**
   ```typescript
   await prisma.webTranslationHistory.create({
     data: {
       webTranslationId,
       key, value, status,
       userId, username, avatarUrl
     }
   });
   ```

### Frontend: Approve Translation

**Location**: `src/app/utils/translationApi.ts`

```typescript
await approveSuggestion(translationId)

POST /api/translations/:id/approve
- Sets status to 'approved'
- Rejects other translations for same key
- Creates history entry
```

### Frontend: Reject Translation

```typescript
await rejectSuggestion(translationId)

DELETE /api/translations/:id
- Soft deletes translation (status = 'rejected')
- Creates history entry
```

---

## Display Flow

### Frontend: Load Translations

**Location**: `src/app/pages/TranslationEditorPage.tsx`

1. **Fetch Project Info**
   ```typescript
   GET /api/projects/:project
   - Get project details
   - Get available languages
   ```

2. **Fetch R2 File (GitHub Import)**
   ```typescript
   const r2Data = await fetchR2File(projectId, lang, filename)
   
   GET /api/r2/:projectId/:lang/:filename
   Returns: {
     raw: { "key": "value" },
     metadata: {
       gitBlame: { "key": { commit, author, email, date } },
       charRanges: { "key": { start: [line, char], end: [line, char] } },
       sourceHashes: { "key": "hash123" }
     },
     sourceHash: "file-hash",
     commitSha: "abc123",
     uploadedAt: "2024-01-01"
   }
   ```

3. **Fetch D1 Translations (Web)**
   ```typescript
   const webTrans = await fetchWebTranslations(projectId, lang, filename)
   
   GET /api/translations
   Query: projectId, language, filename, status, isValid
   Returns: [
     {
       id, projectId, language, filename, key, value,
       status, sourceHash, isValid,
       username, avatarUrl,
       createdAt, updatedAt
     }
   ]
   ```

4. **Merge Data** (`src/app/utils/translationApi.ts`)
   ```typescript
   function mergeTranslations(r2Data, webTranslations) {
     return Object.entries(r2Data.raw).map(([key, sourceValue]) => {
       const webTrans = webTranslations.find(t => t.key === key && t.status === 'approved');
       
       return {
         key,
         sourceValue,          // From R2 (GitHub import)
         currentValue: webTrans?.value || sourceValue,
         gitBlame: r2Data.metadata.gitBlame[key],
         charRange: r2Data.metadata.charRanges[key],
         webTranslation: webTrans,
         isValid: webTrans?.isValid ?? true
       };
     });
   }
   ```

5. **Display in UI**
   ```
   TranslationList:
     - Shows all keys with source values
     - Badges: Web/Git, Valid/Invalid
     - Search and filter
   
   TranslationEditorPanel:
     - Source section: source value, git blame info
     - Translation section: current value, validation status
     - Save button with loading state
   ```

### Backend: Serve R2 File

**Location**: `src/routes/r2-files.ts`

1. **Cache Check**
   ```typescript
   // Check in-memory cache (1 hour TTL)
   const cached = cache.get(r2Key);
   if (cached && !isExpired(cached.timestamp)) {
     return c.json(cached.data, {
       headers: { 'ETag': cached.etag }
     });
   }
   ```

2. **Fetch from R2**
   ```typescript
   const object = await env.KORO_TRANSLATIONS.get(r2Key);
   const packed = await object.arrayBuffer();
   const data = decode(packed); // MessagePack decode
   ```

3. **Cache & Return**
   ```typescript
   const etag = generateETag(data);
   cache.set(r2Key, { data, etag, timestamp });
   
   return c.json(data, {
     headers: {
       'ETag': etag,
       'Cache-Control': 'public, max-age=3600'
     }
   });
   ```

---

## Validation Flow

### Source Change Detection

1. **Upload with New Source Hash**
   ```
   - Client uploads file with new sourceHash
   - Server stores to R2 with new hash
   - Triggers translation invalidation
   ```

2. **Invalidate Outdated Translations**
   ```typescript
   // In project-files.ts, after R2 upload
   await invalidateOutdatedTranslations(projectId, language, filename, newSourceHash);
   
   // Sets isValid = false for translations with old sourceHash
   UPDATE WebTranslation
   SET isValid = false
   WHERE projectId = ? AND language = ? AND filename = ?
     AND sourceHash != ?
   ```

3. **Display in Frontend**
   ```
   - Show âš ï¸ badge for invalid translations
   - Filter by valid/invalid status
   - Highlight outdated translations in editor
   ```

### Validation on Translation Display

**Location**: `src/app/utils/translationApi.ts`

```typescript
function mergeTranslations(r2Data, webTranslations) {
  // For each merged translation:
  const webTrans = webTranslations.find(...);
  
  // Check if translation is valid
  const isValid = webTrans?.isValid ?? true;
  
  return {
    ...,
    isValid,  // Used for UI badges and warnings
    webTranslation: webTrans
  };
}
```

---

## Cleanup Flow

### Automatic Cleanup After Upload

**Triggered**: After single upload or last chunk of chunked upload

1. **Track All Source Files**
   ```typescript
   // Client sends complete list of source files
   allSourceFiles: [
     "en/common.json",
     "ja/common.json",
     "es/common.json"
   ]
   ```

2. **Identify Orphaned Files** (`src/lib/r2-storage.ts`)
   ```typescript
   async function cleanupOrphanedFiles(
     bucket, db, projectId, branch, allSourceFiles
   ) {
     // Query D1 for existing files
     const existing = await db.r2File.findMany({
       where: { projectId, branch }
     });
     
     // Find files in R2 but not in source
     const orphaned = existing.filter(file => {
       const path = `${file.language}/${file.filename}`;
       return !allSourceFiles.includes(path);
     });
     
     return orphaned;
   }
   ```

3. **Delete Orphaned Files**
   ```typescript
   for (const file of orphaned) {
     // Delete from R2
     await bucket.delete(file.r2Key);
     
     // Delete from D1
     await db.r2File.delete({
       where: { id: file.id }
     });
   }
   ```

4. **Return Cleanup Result**
   ```json
   {
     "cleanupResult": {
       "deleted": 3,
       "files": ["en/old-file.json", ...]
     }
   }
   ```

### Cleanup-Only Mode

If differential upload skips all files (nothing changed):

```
- Upload endpoint not called
- Cleanup endpoint still called
- Removes orphaned files
- Returns cleanup result
```

---

## Authentication Flows

### Web UI Authentication (GitHub OAuth)

**Location**: `src/routes/auth.ts`

1. **Initiate OAuth**
   ```
   GET /api/auth/login
   - Generate random state
   - Store in D1 (OAuthState table)
   - Redirect to GitHub OAuth URL
   ```

2. **OAuth Callback**
   ```
   GET /api/auth/callback?code=...&state=...
   - Verify state matches stored state
   - Exchange code for GitHub access token
   - Fetch user info from GitHub
   - Create/update user in D1
   - Generate JWT token
   - Set cookie and redirect to dashboard
   ```

3. **JWT Token Usage**
   ```
   - Frontend sends token in Authorization header
   - Backend verifies JWT signature
   - Extracts user info from token
   - Validates user access
   ```

### GitHub Action Authentication (OIDC)

**Location**: `src/oidc.ts`

1. **OIDC Token from GitHub**
   ```
   - GitHub Actions provides OIDC token
   - Token includes: repository, owner, ref, sha
   - No secrets needed!
   ```

2. **Token Verification**
   ```typescript
   async function verifyOIDCToken(token) {
     // Decode JWT header
     const { kid } = decodeHeader(token);
     
     // Fetch GitHub OIDC public key
     const jwks = await fetch('https://token.actions.githubusercontent.com/.well-known/jwks');
     const publicKey = findKey(jwks, kid);
     
     // Verify signature
     const payload = await verifyJWT(token, publicKey);
     
     // Extract claims
     return {
       repository: payload.repository,
       repositoryOwner: payload.repository_owner,
       ref: payload.ref,
       sha: payload.sha
     };
   }
   ```

3. **Project Access Check**
   ```typescript
   // Verify project matches OIDC repository
   const project = await db.project.findFirst({
     where: {
       name: projectName,
       githubRepo: oidcClaims.repository
     }
   });
   
   if (!project) {
     throw new Error('Project not found or access denied');
   }
   ```

---

## Performance Optimizations

### Client-Side Pre-Packing

**Why**: Cloudflare Workers free tier has 10ms CPU limit

**Implementation**:
```
Client (GitHub Actions - unlimited CPU):
  - MessagePack encode metadata â†’ ~0.5ms per file
  - Base64 encode â†’ ~0.1ms per file
  
Server (Cloudflare Workers - 10ms limit):
  - Base64 decode â†’ ~0.1ms per file
  - Write to R2 â†’ ~0.2ms per file
  - Total: ~0.3ms per file âœ…
```

### Batched D1 Operations

**Why**: Individual upserts are slow (~1ms each)

**Implementation**:
```sql
-- Instead of: 10 individual upserts (10ms)
INSERT INTO R2File (...) VALUES (...)  -- 1ms each Ã— 10 = 10ms

-- Do this: 1 batch insert (2ms)
INSERT INTO R2File (...) VALUES (...), (...), (...)
ON CONFLICT(...) DO UPDATE SET ...  -- 2ms total
```

**Result**: 25x faster for batch operations

### Deferred Translation Invalidation

**Why**: Expensive operation should run once

**Implementation**:
```
Chunks 1-23:
  - R2 write + D1 batch insert (~5ms CPU)
  
Chunk 24 (last):
  - R2 write + D1 batch insert + Invalidation (~7ms CPU)
```

### Differential Upload

**Why**: Skip unchanged files

**Implementation**:
```
1. Fetch existing files with sourceHash
2. Compare hashes locally
3. Skip unchanged files
4. Upload only changes

Result: 90%+ reduction in upload time for typical updates
```

### Caching Strategy

**R2 Files**:
- In-memory cache: 1 hour TTL
- ETag support: 304 Not Modified
- Reduces R2 reads by ~90%

**D1 Queries**:
- No caching (always fresh)
- Optimized with indexes
- Batched operations

---

## Error Handling

### Upload Errors

1. **OIDC Token Invalid**
   ```
   - Verify token signature
   - Check token expiration
   - Validate claims
   - Return 401 Unauthorized
   ```

2. **Project Not Found**
   ```
   - Check project exists
   - Verify OIDC repository matches
   - Return 404 Not Found
   ```

3. **R2 Storage Failure**
   ```
   - Retry R2 write (3 attempts)
   - Log error details
   - Return 500 Internal Server Error
   - Client can retry chunk
   ```

4. **D1 Batch Insert Failure**
   ```
   - Rollback transaction
   - Log error with file details
   - Return 500 Internal Server Error
   - Client retries entire chunk
   ```

### Translation Errors

1. **Source File Not Found**
   ```
   - Check R2 file exists
   - Return 404 Not Found
   - Show error in UI
   ```

2. **Invalid Translation Data**
   ```
   - Validate input fields
   - Return 400 Bad Request
   - Show validation errors in UI
   ```

3. **Concurrent Updates**
   ```
   - Use D1 transactions
   - Handle unique constraint violations
   - Return conflict error
   - UI retries or shows conflict
   ```

---

## Summary

### Key Design Decisions

1. **Separation of Concerns**
   - R2: Immutable GitHub imports with git history
   - D1: Mutable web translations with validation
   - Merge in frontend for display

2. **CPU Optimization**
   - Client-side pre-packing (MessagePack + base64)
   - Batched D1 operations
   - Deferred invalidation
   - Chunked uploads

3. **Differential Upload**
   - Hash-based change detection
   - Skip unchanged files
   - Automatic cleanup

4. **OIDC Authentication**
   - No secrets needed for GitHub Actions
   - Repository-based access control
   - Secure token verification

5. **Validation System**
   - Source hash tracking
   - Auto-invalidation on source changes
   - Visual indicators in UI

### Performance Characteristics

**Free Tier Usage** (per 200 files upload):
- R2 writes: 200 (or ~10 with differential upload)
- R2 reads: ~10 (with caching)
- D1 writes: 200 (or ~10 with differential upload)
- D1 reads: ~100 (list + translations)
- CPU time: ~5ms per chunk (well under 10ms limit)
- Storage: <1GB total

**Upload Times**:
- Full upload (200 files): ~30 seconds
- Differential upload (10 changed): ~5 seconds
- Cleanup: ~2 seconds

**Display Times**:
- R2 fetch: ~50ms (cached) or ~200ms (uncached)
- D1 fetch: ~20ms
- Merge: <1ms (client-side)
- Total: <300ms for full page load
